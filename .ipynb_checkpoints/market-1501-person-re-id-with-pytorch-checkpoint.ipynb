{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb24a58",
   "metadata": {
    "papermill": {
     "duration": 0.002342,
     "end_time": "2025-08-30T08:07:34.672742",
     "exception": false,
     "start_time": "2025-08-30T08:07:34.670400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f455aac3",
   "metadata": {
    "papermill": {
     "duration": 0.001308,
     "end_time": "2025-08-30T08:07:34.675736",
     "exception": false,
     "start_time": "2025-08-30T08:07:34.674428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Market-1501 Person Re-ID with PyTorch**\n",
    "**“Person Re-Identification on Market-1501 using ResNet50 Embeddings”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092a3a3",
   "metadata": {
    "papermill": {
     "duration": 0.001295,
     "end_time": "2025-08-30T08:07:34.678626",
     "exception": false,
     "start_time": "2025-08-30T08:07:34.677331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Pipeline Overview**\n",
    "\n",
    "1. **Dataset Preparation**\n",
    "\n",
    "   * Load **Market-1501** dataset (`bounding_box_train`, `query`, `bounding_box_test`).\n",
    "   * Parse each filename into **person ID (pid)** and **camera ID (camid)**.\n",
    "   * For evaluation, we keep raw pid/camid (no remapping).\n",
    "\n",
    "2. **Model**\n",
    "\n",
    "   * Backbone: **ResNet50** pretrained on ImageNet.\n",
    "   * Remove the classifier; use the last global pooled feature (2048-D).\n",
    "   * Apply **L2 normalization** → features are unit vectors, so similarity = cosine similarity.\n",
    "\n",
    "3. **Feature Extraction**\n",
    "\n",
    "   * Pass **gallery** and **query** images through the model.\n",
    "   * Store feature vectors + their pids, camids, and paths.\n",
    "\n",
    "4. **Evaluation**\n",
    "\n",
    "   * For each query:\n",
    "\n",
    "     * Compute cosine similarity with all gallery features.\n",
    "     * Exclude **same pid & same camera** images (junk).\n",
    "     * Rank gallery by similarity.\n",
    "     * Compute:\n",
    "\n",
    "       * **Rank-1 accuracy** (probability top-1 match is correct).\n",
    "       * **mAP (mean Average Precision)** (quality of entire ranked list).\n",
    "   * Aggregate results across queries.\n",
    "\n",
    "5. **Visualization**\n",
    "\n",
    "   * Pick a few query images.\n",
    "   * Show the **query + Top-K retrieved gallery images**.\n",
    "   * Mark retrieved images as **green (correct pid)** or **red (wrong pid)**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92f061d",
   "metadata": {
    "papermill": {
     "duration": 71.005328,
     "end_time": "2025-08-30T08:08:45.685399",
     "exception": false,
     "start_time": "2025-08-30T08:07:34.680071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model ready.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Market-1501-v15.09.15/bounding_box_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m gallery_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounding_box_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m query_dir   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m gallery_set \u001b[38;5;241m=\u001b[39m \u001b[43mMarket1501Folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgallery_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m query_set   \u001b[38;5;241m=\u001b[39m Market1501Folder(query_dir, transform)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGallery images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(gallery_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mMarket1501Folder.__init__\u001b[0;34m(self, root, transform)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples: List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# (path, pid, camid)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Market-1501-v15.09.15/bounding_box_test'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0. Repro, Device\n",
    "# =========================================================\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. Utilities to parse Market-1501 filename -> (pid, camid)\n",
    "#    Filename looks like: 0002_c1s1_... .jpg\n",
    "# =========================================================\n",
    "def parse_pid_cam(fname: str) -> Tuple[int, int]:\n",
    "    \"\"\"Return (pid, camid). pid=-1 means junk.\"\"\"\n",
    "    pid_str = fname.split('_')[0]\n",
    "    try:\n",
    "        pid = int(pid_str)\n",
    "    except:\n",
    "        pid = -1\n",
    "    # cam code is like 'c1s1', take the '1' after 'c'\n",
    "    parts = fname.split('_')\n",
    "    cam_part = parts[1] if len(parts) > 1 else \"c-1s-1\"\n",
    "    # find integer directly after 'c'\n",
    "    camid = -1\n",
    "    if 'c' in cam_part:\n",
    "        try:\n",
    "            camid = int(cam_part.split('c')[1].split('s')[0])\n",
    "        except:\n",
    "            camid = -1\n",
    "    return pid, camid\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. Dataset classes (gallery/query)\n",
    "# =========================================================\n",
    "class Market1501Folder(Dataset):\n",
    "    \"\"\"\n",
    "    Reads a flat folder of images (e.g., bounding_box_train, bounding_box_test, query).\n",
    "    Keeps raw pid/camid (no label remap), as evaluation uses raw IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.samples: List[Tuple[str, int, int]] = []  # (path, pid, camid)\n",
    "\n",
    "        for fname in os.listdir(root):\n",
    "            if not fname.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            pid, camid = parse_pid_cam(fname)\n",
    "            if pid == -1:  # junk images\n",
    "                continue\n",
    "            self.samples.append((os.path.join(root, fname), pid, camid))\n",
    "\n",
    "        self.samples.sort(key=lambda x: x[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path, pid, camid = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, pid, camid, path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. Transforms\n",
    "# =========================================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),  # (H, W) typical for ReID\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. Model (ResNet50 w/ ID head for training; we only need the features for eval)\n",
    "#    If you already trained, you can load your weights; otherwise we’ll use ImageNet features.\n",
    "# =========================================================\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class ReIDModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=2048):\n",
    "        super().__init__()\n",
    "        base = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base.children())[:-1])  # [B, 2048, 1, 1]\n",
    "        self.embedding_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.feature_extractor(x)        # [B, 2048, 1, 1]\n",
    "        feat = feat.view(feat.size(0), -1)      # [B, 2048]\n",
    "        # Optionally L2-normalize for cosine similarity\n",
    "        feat = nn.functional.normalize(feat, p=2, dim=1)\n",
    "        return feat\n",
    "\n",
    "model = ReIDModel().to(device)\n",
    "model.eval()\n",
    "print(\"Model ready.\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. Datasets (Gallery + Query)\n",
    "#    Adjust dataset_root if needed.\n",
    "# =========================================================\n",
    "dataset_root = \"Market-1501-v15.09.15\"\n",
    "gallery_dir = os.path.join(dataset_root, \"bounding_box_test\")\n",
    "query_dir   = os.path.join(dataset_root, \"query\")\n",
    "\n",
    "gallery_set = Market1501Folder(gallery_dir, transform)\n",
    "query_set   = Market1501Folder(query_dir, transform)\n",
    "\n",
    "print(f\"Gallery images: {len(gallery_set)}\")\n",
    "print(f\"Query images:   {len(query_set)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. Feature extraction helper\n",
    "# =========================================================\n",
    "@torch.no_grad()\n",
    "def extract_features(dset: Dataset, batch_size=64) -> Dict[str, torch.Tensor]:\n",
    "    loader = DataLoader(dset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    all_feats, all_pids, all_camids, all_paths = [], [], [], []\n",
    "    for imgs, pids, camids, paths in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs)\n",
    "        all_feats.append(feats.cpu())\n",
    "        all_pids.extend(pids.tolist())\n",
    "        all_camids.extend(camids.tolist())\n",
    "        all_paths.extend(paths)\n",
    "    feats = torch.cat(all_feats, dim=0)  # [N, 2048]\n",
    "    return {\n",
    "        \"feats\": feats,                           # [N, D]\n",
    "        \"pids\": torch.tensor(all_pids, dtype=torch.long),      # [N]\n",
    "        \"camids\": torch.tensor(all_camids, dtype=torch.long),  # [N]\n",
    "        \"paths\": all_paths\n",
    "    }\n",
    "\n",
    "print(\"Extracting gallery features...\")\n",
    "gallery = extract_features(gallery_set, batch_size=128)\n",
    "print(\"Extracting query features...\")\n",
    "query = extract_features(query_set, batch_size=128)\n",
    "\n",
    "print(\"Feature shapes:\", gallery[\"feats\"].shape, query[\"feats\"].shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. Similarity + Evaluation (CMC / mAP)\n",
    "#    - For each query:\n",
    "#      * compute cosine similarity to gallery\n",
    "#      * sort by similarity desc\n",
    "#      * filter out same pid & same cam (as per Market-1501)\n",
    "#      * compute AP and CMC\n",
    "# =========================================================\n",
    "def compute_ap_cmc(\n",
    "    sim: np.ndarray, gal_pids: np.ndarray, gal_camids: np.ndarray, q_pid: int, q_cam: int\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sim: similarity scores to each gallery item, shape [G]\n",
    "        gal_pids, gal_camids: gallery pid/cam arrays, shape [G]\n",
    "        q_pid, q_cam: query pid/cam\n",
    "    Returns:\n",
    "        ap: average precision for this query\n",
    "        cmc: 0/1 array of length G; CMC curve (first correct match rank onward are 1)\n",
    "    \"\"\"\n",
    "    # Sort gallery by similarity desc\n",
    "    order = np.argsort(-sim)\n",
    "\n",
    "    # Remove gallery samples with same pid & same cam as the query (per standard Market-1501 protocol)\n",
    "    mask_junk = (gal_pids == q_pid) & (gal_camids == q_cam)\n",
    "    # Valid gallery indices after removing junk\n",
    "    keep = ~mask_junk\n",
    "    order = order[keep[order]]\n",
    "\n",
    "    matches = (gal_pids[order] == q_pid).astype(np.int32)  # 1 for correct pid\n",
    "\n",
    "    # If no valid positives, return (0, zeros)\n",
    "    num_rel = matches.sum()\n",
    "    cmc = np.zeros(len(matches), dtype=np.int32)\n",
    "    if num_rel == 0:\n",
    "        return 0.0, cmc\n",
    "\n",
    "    # CMC: find first correct match\n",
    "    first_idx = np.where(matches == 1)[0][0]\n",
    "    cmc[first_idx:] = 1\n",
    "\n",
    "    # mAP: average precision\n",
    "    # precision at each position where match==1\n",
    "    idxs = np.where(matches == 1)[0]\n",
    "    precisions = []\n",
    "    for i, idx in enumerate(idxs, start=1):\n",
    "        # number of correct up to idx / (idx + 1)\n",
    "        prec = matches[:idx+1].sum() / float(idx + 1)\n",
    "        precisions.append(prec)\n",
    "    ap = np.mean(precisions)\n",
    "\n",
    "    return ap, cmc\n",
    "\n",
    "\n",
    "def evaluate(query, gallery, max_queries: int = None) -> Tuple[float, float]:\n",
    "    q_feats = query[\"feats\"].numpy()      # [Q, D]\n",
    "    q_pids  = query[\"pids\"].numpy()\n",
    "    q_cams  = query[\"camids\"].numpy()\n",
    "\n",
    "    g_feats = gallery[\"feats\"].numpy()    # [G, D]\n",
    "    g_pids  = gallery[\"pids\"].numpy()\n",
    "    g_cams  = gallery[\"camids\"].numpy()\n",
    "\n",
    "    # cosine similarity since features are L2-normalized: sim = q @ g.T\n",
    "    sims = np.dot(q_feats, g_feats.T)  # [Q, G]\n",
    "\n",
    "    aps = []\n",
    "    cmc_accum = None\n",
    "\n",
    "    Q = len(q_feats) if max_queries is None else min(max_queries, len(q_feats))\n",
    "    for i in range(Q):\n",
    "        ap, cmc = compute_ap_cmc(sims[i], g_pids, g_cams, q_pids[i], q_cams[i])\n",
    "        aps.append(ap)\n",
    "        if cmc_accum is None:\n",
    "            cmc_accum = cmc.astype(np.float64)\n",
    "        else:\n",
    "            # Align lengths (cmc length equals number of kept gallery after filtering; use full gallery length upper bound)\n",
    "            L = max(len(cmc_accum), len(cmc))\n",
    "            if len(cmc_accum) < L:\n",
    "                cmc_accum = np.pad(cmc_accum, (0, L - len(cmc_accum)))\n",
    "            if len(cmc) < L:\n",
    "                cmc = np.pad(cmc, (0, L - len(cmc)))\n",
    "            cmc_accum += cmc\n",
    "\n",
    "    mAP = float(np.mean(aps) if aps else 0.0)\n",
    "    # Rank-1 is CMC[0] averaged over queries\n",
    "    rank1 = float(cmc_accum[0] / Q if cmc_accum is not None else 0.0)\n",
    "\n",
    "    return mAP, rank1\n",
    "\n",
    "\n",
    "# ---- Evaluate on a small subset for speed (change to None for full) ----\n",
    "mAP_small, rank1_small = evaluate(query, gallery, max_queries=100)  # e.g., first 100 queries\n",
    "print(f\"[Subset Eval] Q=100 → mAP: {mAP_small:.4f}, Rank-1: {rank1_small:.4f}\")\n",
    "\n",
    "# (Optional) Full evaluation (may take longer):\n",
    "# mAP_full, rank1_full = evaluate(query, gallery, max_queries=None)\n",
    "# print(f\"[Full Eval] mAP: {mAP_full:.4f}, Rank-1: {rank1_full:.4f}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. Visualization: show Top-K retrievals for a few queries\n",
    "# =========================================================\n",
    "def visualize_topk(\n",
    "    query, gallery, k: int = 5, num_queries: int = 3, random_pick: bool = True\n",
    "):\n",
    "    q_feats = query[\"feats\"].numpy()\n",
    "    q_pids  = query[\"pids\"].numpy()\n",
    "    q_cams  = query[\"camids\"].numpy()\n",
    "    q_paths = query[\"paths\"]\n",
    "\n",
    "    g_feats = gallery[\"feats\"].numpy()\n",
    "    g_pids  = gallery[\"pids\"].numpy()\n",
    "    g_cams  = gallery[\"camids\"].numpy()\n",
    "    g_paths = gallery[\"paths\"]\n",
    "\n",
    "    sims = np.dot(q_feats, g_feats.T)  # cosine similarity\n",
    "\n",
    "    Q_idx_list = list(range(len(q_feats)))\n",
    "    if random_pick:\n",
    "        random.shuffle(Q_idx_list)\n",
    "    Q_idx_list = Q_idx_list[:num_queries]\n",
    "\n",
    "    for qi in Q_idx_list:\n",
    "        q_pid, q_cam, q_path = q_pids[qi], q_cams[qi], q_paths[qi]\n",
    "\n",
    "        # sort by similarity desc\n",
    "        order = np.argsort(-sims[qi])\n",
    "        # filter junk (same pid & same cam as query)\n",
    "        junk = (g_pids == q_pid) & (g_cams == q_cam)\n",
    "        keep_order = order[~junk[order]]\n",
    "\n",
    "        topk = keep_order[:k]\n",
    "        topk_paths = [g_paths[i] for i in topk]\n",
    "        topk_pids  = g_pids[topk]\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(3*(k+1), 4))\n",
    "\n",
    "        # Query image\n",
    "        ax = plt.subplot(1, k+1, 1)\n",
    "        ax.imshow(Image.open(q_path).convert(\"RGB\"))\n",
    "        ax.set_title(f\"Query\\nPID:{q_pid} Cam:{q_cam}\", fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Top-K gallery results\n",
    "        for j, (p, pid) in enumerate(zip(topk_paths, topk_pids), start=2):\n",
    "            ax = plt.subplot(1, k+1, j)\n",
    "            ax.imshow(Image.open(p).convert(\"RGB\"))\n",
    "            match = (pid == q_pid)\n",
    "            ax.set_title(f\"Top{j-1}\\nPID:{pid}\\n{'Correct' if match else 'Wrong'}\", fontsize=10)\n",
    "            # Optional: a thin border to indicate correct/incorrect\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_linewidth(3.0)\n",
    "                spine.set_edgecolor(\"green\" if match else \"red\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Show three random queries with their Top-5 retrievals\n",
    "visualize_topk(query, gallery, k=5, num_queries=3, random_pick=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9d074",
   "metadata": {
    "papermill": {
     "duration": 0.026928,
     "end_time": "2025-08-30T08:08:45.739630",
     "exception": false,
     "start_time": "2025-08-30T08:08:45.712702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 39965,
     "sourceId": 62075,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79.012723,
   "end_time": "2025-08-30T08:08:48.084118",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-30T08:07:29.071395",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
